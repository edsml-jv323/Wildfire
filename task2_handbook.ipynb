{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download Results data if needed\n",
    "# !curl \"https://drive.usercontent.google.com/download?id={1NvnpyMYoV0GNq0hC0EokHKindbpU65Qs}&confirm=xxx\" -o \"./data/generated_images_VAE.npy\"\n",
    "\n",
    "# # Download trained model if needed\n",
    "# !curl \"https://drive.usercontent.google.com/download?id={10eJvrjtVV6hE2-pNdgun8RIJ8YNQJX81}&confirm=xxx\" -o \"./data/Final_Linear.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from livelossplot import PlotLosses\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import custom packages\n",
    "from WildfireThomas.WildfireGenerate.models import VAE\n",
    "from WildfireThomas.WildfireGenerate.task2functions import training, predict\n",
    "from WildfireThomas.WildfireDA.task3functions import assimilate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path =  'data/Ferguson_fire_train.npy'\n",
    "train_data = np.load(data_path)\n",
    "test_path =  'data/Ferguson_fire_test.npy'\n",
    "test_data = np.load(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for task 2 we decided on was a VAE (variational autoencoder), and it was trained on 3D data (Ferguson_fire.train.npy) including a time steps, where a sequence of 20 time steps were taken from each series of originally 100 time steps, each spaced 5 steps apart. We then created the datasets where the training is the first 19, and the validation is the next (last) 19, which corresponds to t and t+1 for train and test respectively. Therefore, the shapes input into the model are (19,256,256) and outputs (19,256,256), 19 images of the corresponding time steps, and the model is trained to predict what happens 5 time steps later. We similary split the Ferguson_fire_test.npy into t and t+1 in 19 time steps and used that is validation\n",
    "\n",
    " The VAE we decided with was a simple linear one with 3 layers in the encoder and decoder respectively,with a latent size = 64 and KL divergence loss using MSE. We used the adam optimizer with learning rate 0.001. We have tested other structures, including a convolutional VAE, as well as testing with 2D input and inputting images independently without time labelled, but the result was nowhere near as good as MSE 0.148 for validation. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataloader is designed to take a set of sequences of 100 images (our train dataset is 125 sequences), in which images are selected at intervals specified by the split_size value which means a sample is taken from a group of 100 images. This leads to a list of 3D objects of size 19,256,256. Here we create 4 dataloader objects for train (t), train(t+1), test (t), test (t+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 5\n",
    "batch_size = 16\n",
    "seq_length = 100\n",
    "\n",
    "train_loader, train_shifted_loader, test_loader, test_shifted_loader = training.create_dataloaders(train_data, test_data, seq_length, split_size, batch_size)\n",
    "\n",
    "# Check dataset shapes and lengths\n",
    "print(f'Train dataset shape: {train_loader.dataset.shape}')\n",
    "print(f'Test dataset shape: {test_loader.dataset.shape}')\n",
    "\n",
    "del train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initiate the model and summarize the layers. We set a latent dimension of 64, and the channel size is the number of time steps for each series (19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "channel_size = 100//split_size -1\n",
    "latent_dim = 64\n",
    "image_size = 256\n",
    "print(channel_size)\n",
    "model = VAE(latent_dim = latent_dim, \n",
    "                channel_size = channel_size\n",
    "                ).to(device)\n",
    "\n",
    "summary(model, (1, 19, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been previously trained has produced the liveloss plot shown below, beign orinally trained for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'data/Final_Linear.pth'\n",
    "\n",
    "if os.path.exists(model_name):\n",
    "    print(f\"The model {model_name} was already trained.\")\n",
    "    plot_filename = 'data/Task2LogLoss.png'\n",
    "\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(model_name, map_location=torch.device('cpu')))\n",
    "    # model = model_info['model']\n",
    "\n",
    "    # Plot loss plot\n",
    "    loss_plot_image = plt.imread(plot_filename)\n",
    "    plt.imshow(loss_plot_image)\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Training new model.\")\n",
    "    num_epochs = 20\n",
    "    liveloss = PlotLosses()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        train_loss = training.train(model, train_loader, train_shifted_loader)\n",
    "        logs['log loss'] = train_loss.detach().numpy()\n",
    "\n",
    "        val_loss = training.validate(model, test_loader, test_shifted_loader)\n",
    "        logs['val_log loss'] = val_loss.detach().numpy()\n",
    "\n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "\n",
    "    plot_filename = 'data/Task2LogLoss.png'\n",
    "    \n",
    "\n",
    "    torch.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](misc/Task2LogLoss.png \"Log loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract random sample from latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generation worked by creating a random torch sample with the dimensions number of images X latent_dim , this is then put through the model's decoder to produce a chosen number of samples. This is then displayed using the diplay function in the predict module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_samples = predict.predict_samples(model, 2, latent_dim)\n",
    "print(predicted_samples[0].shape)\n",
    "print(predicted_samples[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.display_samples(predicted_samples, channel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display images after applying threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "binary_image = (predicted_samples > threshold).astype(int)\n",
    "predict.display_samples(binary_image, channel_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MSE with Satellite Observed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the notebook `Chosing_Image.ipynb`. In that notebook we built a metric to chose the best AI generated images for each of the satellite and background images.\n",
    "\n",
    "To find the best corresponding time step to the satellite images, we calculate the MSE between each image and computed a metric (refer to  'Chosing_image.ipynb' notebook). The lowest MSE corresponding to the first background image was the 8th generated image (40th time step), and the second satellite image corresponded to the 10th generated image (50th time step), which meant the our model made sense in that it grew in a very similar way to the satellite data as it was observed every 10 time steps. Hence, we chose to use the 8th, 10th, 12th, 14th, and 16th, image for data assimilation. We have also attempted to find the MSE between our model and the satellite data, but found that getting corresponding images to the background was more appropriate for our purpose of a data assimilation to incorporate more information from the model\n",
    "\n",
    "Here we are going to pick those best 5 images and calculate the MSE with the background data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dataset = np.load('data/Ferguson_fire_obs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 256, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_generated_images = np.load('data/generated_images_VAE.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 256, 256)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_generated_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_generated_images = best_generated_images.reshape(5,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 256, 256)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_generated_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07300988149632023"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assimilate.mse(obs_dataset, best_generated_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing the satellite images with our AI-generated images, we achieved a combined Mean Squared Error (MSE) of 0.0746 (sum of 5 images). This low MSE value indicates a strong alignment between the satellite images and the images produced by our model. Essentially, the model's ability to predict the state of the system 5 time steps ahead is quite accurate, as demonstrated by the minimal error in the generated images compared to the actual satellite data. This level of precision suggests that our model effectively captures the underlying patterns and dynamics observed in the satellite imagery, thereby validating the model's performance and its suitability for data assimilation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](misc/VAEnothreshold.png \"VAE with no threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](misc/VAEThreshold.png \"VAE with no threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml4p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
